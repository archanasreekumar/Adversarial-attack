{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128, 64])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 2.099..  Test Loss: 1.495..  Test Accuracy: 0.448\n",
      "Epoch: 1/2..  Training Loss: 1.440..  Test Loss: 0.940..  Test Accuracy: 0.599\n",
      "Epoch: 1/2..  Training Loss: 1.105..  Test Loss: 0.826..  Test Accuracy: 0.636\n",
      "Epoch: 1/2..  Training Loss: 1.012..  Test Loss: 0.781..  Test Accuracy: 0.670\n",
      "Epoch: 1/2..  Training Loss: 0.987..  Test Loss: 0.732..  Test Accuracy: 0.698\n",
      "Epoch: 1/2..  Training Loss: 0.901..  Test Loss: 0.732..  Test Accuracy: 0.690\n",
      "Epoch: 1/2..  Training Loss: 0.848..  Test Loss: 0.686..  Test Accuracy: 0.706\n",
      "Epoch: 1/2..  Training Loss: 0.822..  Test Loss: 0.681..  Test Accuracy: 0.717\n",
      "Epoch: 1/2..  Training Loss: 0.805..  Test Loss: 0.648..  Test Accuracy: 0.731\n",
      "Epoch: 1/2..  Training Loss: 0.809..  Test Loss: 0.671..  Test Accuracy: 0.739\n",
      "Epoch: 1/2..  Training Loss: 0.783..  Test Loss: 0.693..  Test Accuracy: 0.721\n",
      "Epoch: 1/2..  Training Loss: 0.763..  Test Loss: 0.647..  Test Accuracy: 0.750\n",
      "Epoch: 1/2..  Training Loss: 0.781..  Test Loss: 0.606..  Test Accuracy: 0.763\n",
      "Epoch: 1/2..  Training Loss: 0.716..  Test Loss: 0.602..  Test Accuracy: 0.768\n",
      "Epoch: 1/2..  Training Loss: 0.740..  Test Loss: 0.618..  Test Accuracy: 0.769\n",
      "Epoch: 1/2..  Training Loss: 0.692..  Test Loss: 0.597..  Test Accuracy: 0.754\n",
      "Epoch: 1/2..  Training Loss: 0.727..  Test Loss: 0.588..  Test Accuracy: 0.768\n",
      "Epoch: 1/2..  Training Loss: 0.695..  Test Loss: 0.576..  Test Accuracy: 0.779\n",
      "Epoch: 1/2..  Training Loss: 0.653..  Test Loss: 0.618..  Test Accuracy: 0.765\n",
      "Epoch: 1/2..  Training Loss: 0.741..  Test Loss: 0.573..  Test Accuracy: 0.776\n",
      "Epoch: 1/2..  Training Loss: 0.669..  Test Loss: 0.552..  Test Accuracy: 0.783\n",
      "Epoch: 1/2..  Training Loss: 0.639..  Test Loss: 0.555..  Test Accuracy: 0.782\n",
      "Epoch: 1/2..  Training Loss: 0.697..  Test Loss: 0.550..  Test Accuracy: 0.786\n",
      "Epoch: 2/2..  Training Loss: 0.682..  Test Loss: 0.548..  Test Accuracy: 0.785\n",
      "Epoch: 2/2..  Training Loss: 0.668..  Test Loss: 0.578..  Test Accuracy: 0.775\n",
      "Epoch: 2/2..  Training Loss: 0.685..  Test Loss: 0.551..  Test Accuracy: 0.805\n",
      "Epoch: 2/2..  Training Loss: 0.682..  Test Loss: 0.544..  Test Accuracy: 0.807\n",
      "Epoch: 2/2..  Training Loss: 0.671..  Test Loss: 0.536..  Test Accuracy: 0.795\n",
      "Epoch: 2/2..  Training Loss: 0.703..  Test Loss: 0.537..  Test Accuracy: 0.809\n",
      "Epoch: 2/2..  Training Loss: 0.629..  Test Loss: 0.518..  Test Accuracy: 0.815\n",
      "Epoch: 2/2..  Training Loss: 0.600..  Test Loss: 0.534..  Test Accuracy: 0.810\n",
      "Epoch: 2/2..  Training Loss: 0.666..  Test Loss: 0.520..  Test Accuracy: 0.814\n",
      "Epoch: 2/2..  Training Loss: 0.613..  Test Loss: 0.518..  Test Accuracy: 0.816\n",
      "Epoch: 2/2..  Training Loss: 0.626..  Test Loss: 0.519..  Test Accuracy: 0.814\n",
      "Epoch: 2/2..  Training Loss: 0.621..  Test Loss: 0.504..  Test Accuracy: 0.818\n",
      "Epoch: 2/2..  Training Loss: 0.652..  Test Loss: 0.515..  Test Accuracy: 0.816\n",
      "Epoch: 2/2..  Training Loss: 0.634..  Test Loss: 0.533..  Test Accuracy: 0.811\n",
      "Epoch: 2/2..  Training Loss: 0.602..  Test Loss: 0.493..  Test Accuracy: 0.818\n",
      "Epoch: 2/2..  Training Loss: 0.585..  Test Loss: 0.488..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.606..  Test Loss: 0.493..  Test Accuracy: 0.819\n",
      "Epoch: 2/2..  Training Loss: 0.574..  Test Loss: 0.495..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.585..  Test Loss: 0.497..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.579..  Test Loss: 0.491..  Test Accuracy: 0.821\n",
      "Epoch: 2/2..  Training Loss: 0.616..  Test Loss: 0.509..  Test Accuracy: 0.820\n",
      "Epoch: 2/2..  Training Loss: 0.578..  Test Loss: 0.491..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.587..  Test Loss: 0.490..  Test Accuracy: 0.830\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'hidden_layers.3.weight', 'hidden_layers.3.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_size', 'output_size', 'hidden_layers', 'state_dict'])\n",
      "{'input_size': 784, 'output_size': 10, 'hidden_layers': [512, 256, 128, 64], 'state_dict': OrderedDict([('hidden_layers.0.weight', tensor([[-1.7941e-02,  2.7585e-02, -1.2929e-02,  ..., -2.1709e-02,\n",
      "          2.5089e-02, -2.5084e-02],\n",
      "        [-1.4638e-02, -4.0711e-03,  2.9994e-02,  ...,  2.8000e-02,\n",
      "          1.8122e-02,  2.6798e-02],\n",
      "        [ 2.4718e-02,  1.1677e-03, -1.9626e-02,  ..., -2.3092e-03,\n",
      "          1.2714e-05, -1.1347e-02],\n",
      "        ...,\n",
      "        [ 3.4028e-02,  3.2385e-02, -1.0598e-02,  ...,  2.9065e-02,\n",
      "         -2.6025e-02, -3.1783e-02],\n",
      "        [-2.2743e-03, -4.5114e-03, -3.4904e-02,  ...,  1.5659e-02,\n",
      "          1.0233e-02,  4.2702e-04],\n",
      "        [-2.1597e-02,  1.2622e-02, -2.3390e-02,  ...,  4.5242e-03,\n",
      "         -6.5680e-03,  2.8061e-02]])), ('hidden_layers.0.bias', tensor([ 5.5027e-03, -1.8897e-02, -2.6843e-02, -1.9716e-02, -3.2421e-02,\n",
      "        -2.2018e-02, -3.1465e-02,  1.8425e-02,  9.1438e-03,  6.7009e-03,\n",
      "        -1.3668e-02,  3.2150e-02, -2.9765e-02, -2.2029e-02, -2.0635e-02,\n",
      "        -1.5959e-02, -1.4046e-02, -9.3645e-03,  1.8206e-02, -2.0962e-02,\n",
      "         2.9321e-02,  1.7693e-02, -2.3598e-02, -1.5670e-02,  3.3886e-02,\n",
      "        -2.5275e-03,  2.8006e-02,  2.0681e-02, -2.7336e-02,  3.2548e-02,\n",
      "        -9.7961e-03, -1.4492e-02,  2.5717e-02, -3.0070e-02,  3.3425e-02,\n",
      "        -1.7403e-02,  7.0589e-03,  3.4127e-03,  2.5976e-03,  2.4104e-02,\n",
      "        -3.0932e-02,  2.7587e-02, -1.4392e-02,  2.5021e-02, -1.4584e-02,\n",
      "         3.3851e-02, -2.0203e-02,  1.4072e-03, -1.9899e-02, -1.6694e-02,\n",
      "        -2.5073e-02,  1.5115e-02,  1.7751e-02, -2.6644e-02, -3.3358e-02,\n",
      "        -1.5509e-02, -3.4257e-02, -1.3603e-02,  1.3744e-02, -1.2502e-02,\n",
      "        -2.3686e-02, -2.1978e-02, -1.8691e-02, -6.6326e-03,  5.8152e-03,\n",
      "        -2.6374e-02, -4.7333e-03,  2.5598e-02, -1.4242e-02,  1.2143e-02,\n",
      "        -2.8795e-02,  2.4658e-02,  6.0555e-03, -3.5026e-02, -3.5536e-02,\n",
      "         9.9586e-03, -2.1052e-02, -2.7303e-02,  3.5014e-02,  2.8516e-02,\n",
      "        -2.4972e-02,  1.8106e-02, -2.3628e-02, -1.4544e-02, -4.4472e-03,\n",
      "         1.8504e-02,  7.9851e-03, -3.8291e-03, -1.0421e-02,  1.9984e-02,\n",
      "        -1.0663e-02, -3.2467e-02, -3.4357e-02, -1.4007e-02, -2.0370e-04,\n",
      "         2.6065e-02,  1.9928e-03, -3.4441e-02, -2.9385e-03,  1.4891e-02,\n",
      "         2.5828e-02,  1.6088e-02,  3.0830e-02, -1.6386e-02, -2.0123e-02,\n",
      "        -3.0195e-03,  2.0955e-02, -2.0571e-02,  3.1001e-02,  4.1416e-03,\n",
      "         3.5400e-02,  5.7050e-03,  2.2985e-02,  8.4581e-03,  2.5893e-03,\n",
      "        -1.5189e-02,  3.1746e-02,  4.0073e-03,  1.3392e-02,  8.2492e-03,\n",
      "         6.2459e-03,  2.8906e-02,  1.0728e-02,  3.2603e-03, -3.0411e-02,\n",
      "         2.2607e-02, -1.6179e-02, -2.1337e-02,  2.8955e-02, -3.3822e-02,\n",
      "         1.1511e-02, -9.0288e-03, -1.7037e-02,  3.0093e-03, -3.3050e-02,\n",
      "        -3.2367e-02,  2.4227e-03,  4.3598e-03, -2.2259e-02, -9.0655e-04,\n",
      "         2.7394e-02, -2.0025e-02, -2.9766e-02, -2.1709e-03, -2.1822e-02,\n",
      "         9.5161e-03,  2.5694e-02,  3.1240e-02,  1.7162e-02,  4.1660e-03,\n",
      "        -7.3648e-03,  4.2227e-03, -6.6807e-03, -1.0226e-02, -1.4590e-02,\n",
      "         3.2821e-02,  2.2232e-02,  1.0927e-02, -4.3110e-03,  8.1755e-05,\n",
      "        -2.3889e-02, -1.7332e-02,  2.9543e-02,  7.0154e-04, -1.2755e-02,\n",
      "        -3.3615e-02, -1.5494e-02,  1.5637e-02,  1.6353e-02, -4.1352e-03,\n",
      "         1.1459e-02, -1.2176e-02, -1.4898e-02, -3.9592e-03, -1.5228e-02,\n",
      "        -2.2937e-02,  6.0231e-03,  2.5883e-02,  2.6261e-02,  3.5336e-02,\n",
      "         1.1978e-02,  2.1966e-02, -5.5874e-03, -1.1245e-02, -9.2043e-03,\n",
      "        -1.2347e-02, -5.1791e-03,  1.9687e-03, -3.5092e-02, -4.8463e-03,\n",
      "         7.6908e-04, -1.3574e-02,  1.6778e-02,  5.1808e-03, -2.0782e-02,\n",
      "        -1.6068e-02, -1.4272e-02, -3.2173e-02,  1.8053e-02, -3.2021e-02,\n",
      "        -2.2645e-02, -1.2106e-02,  2.9959e-02,  2.5821e-03,  2.1568e-02,\n",
      "         2.3707e-02, -7.6861e-03,  2.5406e-02,  2.0636e-02,  3.8653e-03,\n",
      "         8.3467e-03,  2.9321e-02, -1.4365e-03,  2.1328e-02,  3.0212e-03,\n",
      "        -3.5056e-02,  2.1149e-02, -1.8541e-02,  7.9611e-03,  1.1885e-02,\n",
      "         4.3188e-03, -3.1642e-02,  2.7209e-02, -2.8543e-02, -4.7035e-03,\n",
      "         3.3957e-02, -1.0746e-02,  5.5185e-03,  2.6913e-02,  6.2585e-03,\n",
      "         1.1754e-02,  2.7441e-02,  1.5242e-02, -3.1133e-02,  9.3825e-03,\n",
      "        -2.4382e-02,  3.1336e-02, -1.0327e-03,  4.1058e-03, -3.9906e-03,\n",
      "         1.5296e-02,  1.2567e-02, -2.0532e-02, -2.0316e-02,  2.3888e-02,\n",
      "         3.0422e-02, -1.8683e-02,  2.7342e-02, -3.3677e-03, -2.2015e-02,\n",
      "         1.1705e-02, -7.7955e-03, -7.9554e-03,  9.9206e-03,  4.9464e-04,\n",
      "         2.2974e-02, -2.4284e-03,  3.2561e-02, -2.1887e-02,  1.9454e-02,\n",
      "         3.3256e-02,  2.5327e-02,  3.3473e-02, -1.4281e-02, -2.0550e-02,\n",
      "         1.1535e-02,  8.1599e-03, -2.6227e-02,  2.9723e-02,  2.5766e-03,\n",
      "        -3.3319e-02, -1.4876e-02,  1.0479e-02, -2.8924e-02,  2.1720e-02,\n",
      "         1.4881e-02,  1.5783e-02,  1.9712e-02,  2.3304e-02, -2.5666e-02,\n",
      "         2.8461e-02,  1.4556e-03,  2.0486e-02,  1.6572e-02, -1.6138e-02,\n",
      "        -1.8382e-02, -1.5152e-02,  3.2087e-03,  3.2887e-03, -2.7218e-02,\n",
      "         4.5734e-03, -2.7422e-02, -3.0686e-02,  1.4403e-02,  2.8382e-03,\n",
      "         2.9363e-02,  2.3954e-02, -3.1280e-02,  1.3385e-03,  5.0100e-03,\n",
      "        -2.9170e-02, -1.3115e-02,  5.1927e-03,  3.3089e-02,  3.3337e-02,\n",
      "         2.3212e-02,  2.7327e-02, -5.3190e-03,  3.5220e-02,  1.0308e-02,\n",
      "        -1.1269e-02,  8.8432e-03,  1.8477e-02,  6.0127e-03,  1.0458e-02,\n",
      "        -1.7175e-03, -5.9447e-03,  1.7405e-02, -1.4619e-02, -3.2174e-02,\n",
      "         2.5180e-02, -2.8295e-02,  2.5993e-02,  1.9411e-02, -3.5693e-02,\n",
      "        -3.4327e-02, -2.8786e-03,  3.1922e-02,  2.7355e-02,  3.3279e-02,\n",
      "         2.3154e-03,  2.6118e-02, -2.1938e-02, -1.5825e-02,  1.0088e-02,\n",
      "        -2.8023e-02,  3.4003e-02, -1.7720e-02, -2.6601e-02, -8.2349e-03,\n",
      "        -4.7811e-03,  3.2664e-02, -3.0669e-02, -3.2368e-02, -2.5195e-02,\n",
      "        -3.1843e-03, -2.0214e-02, -2.3495e-02, -1.7075e-02, -1.4178e-02,\n",
      "        -5.4540e-03,  6.9403e-03, -2.9371e-02, -3.9843e-03,  8.5490e-03,\n",
      "        -7.8156e-03,  2.8053e-04,  3.3279e-02,  3.1109e-02,  2.6253e-02,\n",
      "         3.1180e-02, -2.8342e-02, -2.6349e-02, -1.7403e-02,  1.0758e-02,\n",
      "        -1.4585e-02,  3.1060e-03,  2.6151e-02, -3.2746e-03,  1.1145e-02,\n",
      "        -1.8294e-02,  1.9521e-02, -3.1884e-02, -2.2938e-02,  2.9954e-02,\n",
      "         3.3258e-02,  2.1241e-02,  1.4699e-02, -1.7740e-02,  2.3277e-02,\n",
      "         1.9900e-02, -2.0043e-02, -3.1023e-02,  2.5617e-02,  1.9367e-02,\n",
      "        -1.3763e-02, -3.2678e-03,  1.4972e-02, -3.0505e-02,  2.8440e-02,\n",
      "        -2.1997e-02, -5.8777e-03,  2.4693e-02,  1.4218e-02, -3.5344e-02,\n",
      "         2.5937e-02,  2.2056e-02, -2.1923e-03,  1.4083e-02,  2.0682e-02,\n",
      "        -1.3441e-02,  2.8770e-02, -1.2913e-02,  2.0233e-02, -2.0647e-02,\n",
      "         2.9406e-02, -2.7120e-02,  3.5678e-02,  1.9006e-02,  1.3719e-02,\n",
      "         3.0953e-02, -1.8363e-02, -3.1385e-02, -2.5744e-02, -1.8548e-02,\n",
      "         3.5463e-02,  3.4157e-02,  2.5564e-02,  1.9952e-02, -2.9212e-02,\n",
      "        -2.8604e-02, -3.1656e-02, -2.4657e-02,  9.2194e-03, -2.5343e-02,\n",
      "         3.1125e-02,  3.3530e-02, -2.9217e-02, -1.4521e-02, -2.2703e-02,\n",
      "        -2.5096e-03,  1.3734e-02,  2.7010e-03,  2.8936e-02, -2.8986e-02,\n",
      "        -3.0167e-02,  1.4048e-02, -8.6918e-03, -8.5622e-03, -3.2463e-02,\n",
      "         7.0866e-03, -2.1470e-02, -3.2147e-02, -2.4857e-02, -2.6297e-02,\n",
      "         2.9082e-02,  2.9258e-03,  1.0952e-02, -2.9360e-02,  7.3201e-03,\n",
      "        -2.8041e-02, -9.7082e-03, -2.8704e-02, -2.2401e-02,  4.1858e-03,\n",
      "        -2.3151e-03,  8.6891e-03,  2.6806e-02,  1.9846e-02,  1.5267e-02,\n",
      "        -2.8389e-02, -9.2918e-03,  2.9917e-02,  8.8037e-03, -3.4887e-02,\n",
      "         2.6288e-02, -3.2453e-02, -1.7188e-02, -1.6484e-02, -2.9504e-02,\n",
      "        -1.6455e-02,  1.1969e-02,  1.7310e-02,  6.2173e-03, -7.2869e-03,\n",
      "         2.9644e-02,  5.5998e-03,  3.6453e-04, -3.5539e-02,  2.8769e-02,\n",
      "         1.0464e-02,  2.6625e-02,  1.7357e-02, -1.7485e-02, -1.8317e-03,\n",
      "         1.3205e-03, -3.0738e-02, -2.9967e-02, -3.2868e-02, -3.1767e-02,\n",
      "        -3.3392e-02,  4.4545e-03, -1.1906e-02, -1.3482e-02,  2.7643e-02,\n",
      "        -4.7293e-03,  1.9716e-02, -1.1648e-02, -1.3202e-02, -5.9253e-03,\n",
      "        -8.3071e-03,  4.8668e-03,  1.3481e-02, -4.0191e-03,  1.5749e-02,\n",
      "        -1.4381e-03, -3.3063e-02, -2.1463e-02,  2.1443e-03, -2.3700e-02,\n",
      "        -1.1349e-02,  2.4509e-02])), ('hidden_layers.1.weight', tensor([[ 0.0441,  0.0230, -0.0384,  ...,  0.0360,  0.0406, -0.0195],\n",
      "        [ 0.0285, -0.0403, -0.0179,  ...,  0.0337, -0.0285,  0.0403],\n",
      "        [-0.0211, -0.0002,  0.0229,  ..., -0.0345, -0.0045, -0.0056],\n",
      "        ...,\n",
      "        [-0.0245, -0.0332, -0.0344,  ...,  0.0023, -0.0348, -0.0346],\n",
      "        [ 0.0296,  0.0304, -0.0206,  ...,  0.0044, -0.0161,  0.0332],\n",
      "        [-0.0125, -0.0200,  0.0156,  ..., -0.0126, -0.0047, -0.0428]])), ('hidden_layers.1.bias', tensor([-4.2690e-02, -3.9716e-02,  1.5311e-02, -1.8577e-02,  2.2670e-02,\n",
      "         3.2874e-03, -2.4324e-02, -1.9611e-02, -4.1687e-02, -1.4044e-02,\n",
      "         1.2339e-02,  1.7576e-03,  9.5277e-03, -4.0560e-02, -3.5265e-02,\n",
      "         1.9248e-02, -3.9391e-02,  1.4997e-02,  5.7711e-03, -4.1357e-02,\n",
      "        -3.0417e-02,  3.3839e-03,  3.0668e-02, -3.3677e-02,  4.1973e-02,\n",
      "        -1.8620e-02,  4.1028e-02, -1.5950e-02,  2.5405e-03, -1.6244e-02,\n",
      "         2.0138e-02, -2.8862e-02,  1.1167e-02,  3.1863e-02, -3.9619e-02,\n",
      "        -1.3879e-02, -2.5318e-02,  3.1013e-02, -3.1352e-02, -1.8920e-02,\n",
      "        -1.5291e-02, -8.1256e-03,  3.2111e-02, -1.9308e-02, -2.5894e-02,\n",
      "        -1.9825e-03, -3.0077e-02, -2.1705e-02, -6.4309e-03,  1.7740e-02,\n",
      "         2.8042e-02, -1.9983e-02,  9.0049e-03,  4.0336e-02,  1.4383e-03,\n",
      "         3.2114e-03, -1.9123e-02,  2.2651e-02,  1.0729e-02,  2.3121e-02,\n",
      "         3.8665e-02,  3.6166e-02, -2.3566e-02, -3.9665e-02, -3.9835e-02,\n",
      "        -3.9107e-02,  3.5265e-02,  2.9113e-02, -5.9547e-03,  3.4988e-02,\n",
      "         4.2794e-02,  2.3209e-02, -1.4255e-02,  2.7041e-02,  1.1411e-02,\n",
      "         3.8610e-02, -8.9676e-03,  1.0233e-02, -2.0430e-02, -4.3652e-02,\n",
      "        -2.1519e-02,  1.3922e-02, -2.1822e-02,  5.8040e-06, -3.0417e-02,\n",
      "         8.6347e-03,  3.1937e-02, -1.0473e-02,  3.4048e-02, -7.5471e-03,\n",
      "         2.9825e-02, -2.6498e-02,  1.0461e-02, -1.7678e-02,  3.0487e-02,\n",
      "         5.2129e-03,  3.0954e-02, -4.2805e-03, -1.7655e-02, -2.4235e-02,\n",
      "        -1.5815e-03, -2.0821e-02, -3.8021e-02, -3.6020e-02,  1.1651e-02,\n",
      "         2.2024e-02,  1.2654e-02, -1.0471e-02, -7.9198e-03, -7.8639e-03,\n",
      "        -2.8947e-02,  2.8705e-02,  2.1434e-02, -4.4023e-02, -2.0415e-02,\n",
      "         3.0694e-02, -3.5800e-02, -4.1514e-02,  2.8668e-02,  6.8237e-03,\n",
      "        -3.8677e-02, -3.8010e-02, -4.8002e-03, -2.0668e-02, -2.6185e-02,\n",
      "         3.4727e-02, -2.3230e-02, -3.9918e-02,  1.7525e-02,  1.3486e-02,\n",
      "         1.3078e-02,  1.9113e-03, -2.8863e-03, -1.9463e-02, -3.6292e-03,\n",
      "        -2.8637e-02,  2.1618e-02,  2.4385e-02, -1.7869e-02,  2.9519e-02,\n",
      "         2.5832e-02, -1.0671e-02,  3.6465e-02, -4.2330e-04,  2.0246e-02,\n",
      "         6.6081e-03,  4.6055e-03, -1.9775e-02,  1.1951e-02, -6.8836e-03,\n",
      "        -3.3485e-02, -2.6411e-02, -4.1752e-03,  3.7348e-03,  4.1418e-02,\n",
      "        -1.0736e-02, -3.8145e-02,  4.0374e-02,  3.5210e-02,  1.7201e-03,\n",
      "         4.0672e-02,  3.0206e-02,  1.8025e-02,  3.7940e-02, -2.0775e-02,\n",
      "        -2.9635e-02,  1.8199e-02,  8.2459e-03, -2.7623e-02, -6.5691e-04,\n",
      "         7.1539e-03,  1.6221e-02,  2.4493e-02,  2.6282e-02,  2.0084e-02,\n",
      "        -3.0942e-02, -4.2327e-02,  1.2298e-02, -1.1587e-02,  2.1880e-02,\n",
      "        -1.8882e-02,  3.6421e-04, -2.2074e-02,  4.5399e-04, -4.9712e-03,\n",
      "         2.4254e-02, -2.3638e-02, -4.2272e-02,  2.4907e-02,  3.7047e-02,\n",
      "        -4.2072e-02, -8.0005e-03,  1.2008e-02,  1.8907e-03, -3.1503e-02,\n",
      "        -1.6833e-02, -1.3174e-02, -1.7118e-02, -5.7295e-03,  2.7713e-02,\n",
      "         3.5253e-02,  2.5507e-02, -2.4315e-02,  9.9845e-03,  1.0204e-02,\n",
      "        -6.6868e-03, -2.3355e-02,  1.4775e-02, -1.0114e-02,  1.0514e-02,\n",
      "         6.1586e-03, -1.3923e-02,  7.1380e-03, -3.6451e-02,  4.2707e-02,\n",
      "        -2.4484e-02, -3.8295e-03,  3.6999e-02, -2.8082e-02,  3.5135e-02,\n",
      "        -3.9720e-02,  1.1229e-02,  1.0501e-02,  3.5285e-03,  2.0852e-02,\n",
      "        -2.0224e-02,  2.6697e-02, -3.4704e-02,  1.1709e-02,  3.9716e-02,\n",
      "         3.6470e-02, -2.2406e-02,  3.2179e-02, -2.9767e-02, -2.0714e-02,\n",
      "         2.5081e-02,  8.5300e-03,  3.8566e-02,  3.5582e-02, -6.8575e-03,\n",
      "         3.0367e-02,  3.0291e-03,  7.2557e-03,  3.4995e-02, -1.1783e-02,\n",
      "        -9.5793e-03, -3.6384e-03,  4.1355e-02, -2.8201e-02,  2.9215e-02,\n",
      "        -2.1290e-03,  2.3063e-02,  3.5306e-03,  1.5349e-03, -2.9632e-02,\n",
      "         2.0495e-02])), ('hidden_layers.2.weight', tensor([[ 0.0410, -0.0137,  0.0406,  ..., -0.0491, -0.0205, -0.0181],\n",
      "        [-0.0044,  0.0385, -0.0091,  ...,  0.0419, -0.0394,  0.0227],\n",
      "        [-0.0553, -0.0164,  0.0501,  ..., -0.0337,  0.0424, -0.0017],\n",
      "        ...,\n",
      "        [-0.0472,  0.0383,  0.0144,  ...,  0.0495,  0.0572, -0.0047],\n",
      "        [ 0.0427, -0.0196,  0.0273,  ...,  0.0238,  0.0539,  0.0570],\n",
      "        [ 0.0072,  0.0542,  0.0364,  ...,  0.0595, -0.0170,  0.0515]])), ('hidden_layers.2.bias', tensor([-0.0237, -0.0325, -0.0275, -0.0149,  0.0120,  0.0019,  0.0527, -0.0592,\n",
      "        -0.0398, -0.0234, -0.0344, -0.0515,  0.0503, -0.0039,  0.0491,  0.0184,\n",
      "         0.0326,  0.0495,  0.0488, -0.0128, -0.0482,  0.0609,  0.0334,  0.0091,\n",
      "        -0.0502,  0.0290,  0.0240,  0.0538, -0.0565,  0.0218,  0.0062,  0.0541,\n",
      "         0.0234, -0.0472,  0.0309, -0.0099,  0.0195,  0.0370, -0.0135,  0.0059,\n",
      "        -0.0600,  0.0370, -0.0539, -0.0585,  0.0157,  0.0369,  0.0232, -0.0095,\n",
      "         0.0053, -0.0513, -0.0194, -0.0314, -0.0130, -0.0158,  0.0432,  0.0008,\n",
      "         0.0465,  0.0248,  0.0443, -0.0288,  0.0070, -0.0265,  0.0103, -0.0060,\n",
      "        -0.0061, -0.0294,  0.0062,  0.0209,  0.0006, -0.0196,  0.0140, -0.0419,\n",
      "         0.0620, -0.0418,  0.0153, -0.0012,  0.0181, -0.0089,  0.0285, -0.0567,\n",
      "        -0.0308,  0.0263, -0.0255, -0.0342,  0.0535, -0.0161,  0.0364, -0.0476,\n",
      "         0.0102, -0.0067, -0.0219,  0.0471,  0.0617, -0.0146, -0.0107,  0.0260,\n",
      "        -0.0466,  0.0557,  0.0495, -0.0271,  0.0115, -0.0226,  0.0383,  0.0243,\n",
      "         0.0208, -0.0448, -0.0621,  0.0114, -0.0336, -0.0173, -0.0573,  0.0582,\n",
      "        -0.0619,  0.0426,  0.0353,  0.0091, -0.0326,  0.0523, -0.0591,  0.0427,\n",
      "         0.0047,  0.0101,  0.0541,  0.0362,  0.0623, -0.0507, -0.0542,  0.0205])), ('hidden_layers.3.weight', tensor([[ 0.0540,  0.0446, -0.0095,  ...,  0.0013, -0.0324, -0.0081],\n",
      "        [ 0.0164,  0.0230,  0.0783,  ...,  0.0363,  0.0440,  0.0142],\n",
      "        [-0.0390, -0.0719, -0.0513,  ...,  0.0191,  0.0739, -0.0781],\n",
      "        ...,\n",
      "        [-0.0318,  0.0234,  0.0108,  ..., -0.0766,  0.0776,  0.0067],\n",
      "        [ 0.0874, -0.0615, -0.0555,  ..., -0.0528,  0.0650,  0.0880],\n",
      "        [-0.0457,  0.0315, -0.0820,  ...,  0.0503,  0.0840, -0.0570]])), ('hidden_layers.3.bias', tensor([-0.0599, -0.0630,  0.0250, -0.0165,  0.0246,  0.0307,  0.0413,  0.0569,\n",
      "         0.0278,  0.0157, -0.0091,  0.0657,  0.0766, -0.0242,  0.0478, -0.0498,\n",
      "        -0.0544,  0.0095,  0.0216, -0.0333,  0.0812,  0.0363,  0.0802, -0.0372,\n",
      "        -0.0553, -0.0818,  0.0463, -0.0124,  0.0475, -0.0749, -0.0872, -0.0239,\n",
      "         0.0010, -0.0411, -0.0018, -0.0166,  0.0878,  0.0692,  0.0501, -0.0345,\n",
      "         0.0440,  0.0489, -0.0004, -0.0525, -0.0334,  0.0811, -0.0496, -0.0785,\n",
      "         0.0336, -0.0057,  0.0741,  0.0725, -0.0633, -0.0047,  0.0865, -0.0117,\n",
      "        -0.0602,  0.0795, -0.0731,  0.0364, -0.0573,  0.0700,  0.0223,  0.0365])), ('output.weight', tensor([[-0.0666, -0.0927,  0.0395, -0.0754,  0.0610, -0.0983,  0.0395, -0.1213,\n",
      "         -0.0278,  0.0744,  0.0425, -0.0475,  0.0973,  0.0171,  0.0123, -0.0114,\n",
      "          0.0451,  0.0486,  0.0173, -0.0070,  0.0453, -0.0719,  0.0472,  0.0129,\n",
      "          0.0052, -0.0841, -0.1142, -0.0943, -0.0823,  0.0433, -0.0069,  0.0100,\n",
      "          0.1210,  0.0729,  0.0854, -0.0553,  0.0043,  0.1110,  0.0937,  0.0464,\n",
      "          0.0244,  0.0885,  0.0466,  0.0323,  0.0034,  0.0042,  0.0455, -0.0816,\n",
      "          0.0632, -0.1027,  0.0527,  0.0074,  0.1008, -0.0444,  0.0353,  0.0608,\n",
      "         -0.0272,  0.0312,  0.1187, -0.0580, -0.0078,  0.0477,  0.0584,  0.0297],\n",
      "        [-0.0752,  0.0951,  0.0141, -0.0775, -0.0136, -0.0338,  0.1095,  0.0374,\n",
      "          0.0635, -0.1039,  0.0069, -0.0149,  0.0796,  0.0480,  0.0983, -0.0902,\n",
      "          0.0931, -0.0075, -0.0302, -0.0679, -0.0586,  0.0991,  0.0892, -0.1190,\n",
      "          0.0080,  0.0402, -0.0066,  0.0919, -0.0174, -0.0801,  0.0763,  0.0788,\n",
      "         -0.1040, -0.0115,  0.0204,  0.0502, -0.0219,  0.0084, -0.0066,  0.1246,\n",
      "          0.0142,  0.0742,  0.0694,  0.0607, -0.0892, -0.0379, -0.0656, -0.0345,\n",
      "          0.0201, -0.0980,  0.0888,  0.0184, -0.0036,  0.0476, -0.1223, -0.0649,\n",
      "          0.0468,  0.0676,  0.0200,  0.0938, -0.0073,  0.0818,  0.0881,  0.0253],\n",
      "        [ 0.0109, -0.0676, -0.0565, -0.1111, -0.0230, -0.0403, -0.0349,  0.0216,\n",
      "         -0.0304, -0.0785, -0.1183,  0.0330,  0.0440,  0.0905,  0.0559,  0.1215,\n",
      "          0.0143, -0.0086, -0.0056, -0.0822,  0.0941,  0.1012, -0.0412,  0.0916,\n",
      "          0.0083, -0.0310,  0.0807, -0.0797,  0.0220, -0.0393, -0.1054, -0.0802,\n",
      "          0.0602, -0.0918, -0.0717,  0.0291, -0.0397, -0.0253, -0.0490,  0.0671,\n",
      "          0.0218,  0.0124, -0.1013, -0.0025,  0.0194, -0.0236,  0.0268, -0.0171,\n",
      "          0.0418, -0.0885,  0.0825, -0.0363, -0.1170,  0.1240, -0.1168, -0.0507,\n",
      "          0.0149,  0.0644, -0.0147, -0.0170,  0.0046, -0.0167,  0.1140, -0.0575],\n",
      "        [ 0.1209,  0.1162,  0.0587, -0.0619,  0.0237,  0.0676, -0.0110,  0.0996,\n",
      "          0.0337, -0.1054,  0.1221,  0.0306, -0.0387, -0.0640, -0.0049,  0.0869,\n",
      "          0.0305,  0.1107, -0.0555, -0.0108,  0.1036,  0.0437, -0.0488, -0.0331,\n",
      "          0.0887, -0.0794, -0.0202, -0.1026,  0.0448,  0.0081, -0.0870, -0.0198,\n",
      "          0.0598, -0.0419, -0.0282, -0.0317,  0.1097, -0.0987,  0.0868, -0.0727,\n",
      "         -0.1192, -0.0560,  0.0826,  0.1198, -0.0036,  0.0461, -0.0859, -0.1225,\n",
      "          0.1195,  0.0347, -0.1215, -0.0734,  0.0204,  0.0864, -0.0073,  0.0591,\n",
      "          0.0814, -0.1231,  0.0784, -0.0306,  0.0556, -0.0896, -0.1215, -0.0589],\n",
      "        [ 0.0633, -0.0679,  0.0286,  0.0116, -0.0343,  0.0016, -0.0670,  0.0410,\n",
      "         -0.0586,  0.0528,  0.1033, -0.0620, -0.0602,  0.0771, -0.0345,  0.1243,\n",
      "          0.0664,  0.0746, -0.0413,  0.0766,  0.0044, -0.0365, -0.0913, -0.0993,\n",
      "         -0.0899,  0.0479,  0.0287, -0.0409, -0.0246,  0.0680, -0.0893, -0.0760,\n",
      "         -0.1218,  0.0543, -0.0485,  0.0858,  0.0936,  0.0815,  0.0238,  0.0968,\n",
      "          0.0393, -0.0360,  0.0105, -0.0402, -0.0524, -0.1000,  0.0775, -0.0216,\n",
      "         -0.0775,  0.0526,  0.0302, -0.1144, -0.0856,  0.0036, -0.0192,  0.0859,\n",
      "          0.1045, -0.0351,  0.0457, -0.0016, -0.0417,  0.1213, -0.0049, -0.1185],\n",
      "        [ 0.0637,  0.0747, -0.1033,  0.1101,  0.0816, -0.0076, -0.1190, -0.0621,\n",
      "         -0.0968, -0.0162, -0.0222, -0.1094,  0.0116, -0.1174,  0.0032, -0.0308,\n",
      "         -0.0002, -0.0915, -0.0614,  0.0749,  0.0788,  0.1157,  0.1062, -0.0753,\n",
      "          0.1116,  0.1054,  0.1198, -0.0645,  0.0031,  0.1226,  0.1058, -0.0990,\n",
      "         -0.0162, -0.0947, -0.0026, -0.0538,  0.0603, -0.1134,  0.0769,  0.1001,\n",
      "          0.0363, -0.1172, -0.1118,  0.1157,  0.0844, -0.0066, -0.0154, -0.1228,\n",
      "         -0.0945,  0.0115, -0.0211,  0.1135,  0.0054, -0.0653, -0.0247, -0.0435,\n",
      "          0.1206, -0.0755, -0.0439, -0.0027, -0.0711,  0.0265, -0.1113,  0.0635],\n",
      "        [-0.0881,  0.0907,  0.0827,  0.0172,  0.0201,  0.0575,  0.1063, -0.1082,\n",
      "         -0.0634, -0.1092, -0.0792, -0.0206, -0.0340,  0.0088, -0.0192,  0.0282,\n",
      "         -0.0536, -0.0194,  0.1176, -0.0705, -0.0434, -0.0877, -0.0590, -0.0417,\n",
      "          0.0572, -0.1086,  0.1080, -0.0613,  0.0738,  0.0144, -0.1200, -0.0759,\n",
      "          0.0439,  0.0266,  0.0275, -0.0650, -0.0694,  0.0991,  0.0059, -0.0132,\n",
      "         -0.0169, -0.1066, -0.0933,  0.0950,  0.0611, -0.0617, -0.1217,  0.0792,\n",
      "         -0.0155,  0.0825, -0.1071, -0.0727, -0.0092, -0.0525,  0.0075, -0.0119,\n",
      "          0.0044,  0.1188, -0.0257,  0.0473, -0.1176, -0.0697, -0.0751,  0.0844],\n",
      "        [-0.0761, -0.0142,  0.1011, -0.0931, -0.0880, -0.0594,  0.1180, -0.0859,\n",
      "         -0.0294,  0.0987,  0.0923,  0.0010,  0.1169, -0.0791, -0.0968,  0.0042,\n",
      "          0.1117, -0.0141, -0.0655, -0.1219, -0.1171,  0.0390,  0.1052,  0.0095,\n",
      "          0.0974,  0.1039,  0.0424, -0.0235, -0.0608, -0.0113,  0.1223, -0.0900,\n",
      "         -0.0600, -0.0519,  0.0512,  0.0753, -0.0136,  0.0087, -0.0375,  0.0017,\n",
      "         -0.0037,  0.1057, -0.0542,  0.1048,  0.0756, -0.0533,  0.1059,  0.0706,\n",
      "          0.0734, -0.0806,  0.0977, -0.0238, -0.0296, -0.1188, -0.0310, -0.0290,\n",
      "         -0.0471,  0.0017,  0.0067, -0.1056,  0.0010, -0.0805,  0.0871, -0.0321],\n",
      "        [-0.0625, -0.0050,  0.0609, -0.0816,  0.0013,  0.1229, -0.0435, -0.1127,\n",
      "         -0.1049,  0.0961, -0.1144, -0.1109, -0.0833,  0.0941, -0.0675,  0.0856,\n",
      "          0.0170,  0.0170, -0.1004,  0.0219, -0.1191, -0.0627,  0.0264,  0.0219,\n",
      "          0.0072, -0.1090, -0.0246, -0.0912,  0.0180, -0.0144, -0.0774, -0.0531,\n",
      "         -0.0400,  0.0971,  0.0361,  0.0117,  0.1239, -0.1072, -0.0438, -0.0623,\n",
      "         -0.0795, -0.0668,  0.0940, -0.0983, -0.1140, -0.1234, -0.0268, -0.0474,\n",
      "          0.0420,  0.1188, -0.0934,  0.0723,  0.0211, -0.1091,  0.1094,  0.1178,\n",
      "         -0.0058, -0.0760, -0.0371, -0.1130, -0.1044, -0.0485,  0.0948,  0.0328],\n",
      "        [-0.0646, -0.0418,  0.0880, -0.0780, -0.0816, -0.0072, -0.1108, -0.0807,\n",
      "         -0.0960,  0.0115, -0.1149,  0.1141,  0.0795,  0.0444, -0.1157,  0.0743,\n",
      "         -0.0520,  0.0201, -0.1248, -0.0034,  0.1169,  0.0122,  0.0786,  0.1143,\n",
      "          0.0640, -0.1065,  0.0641, -0.0960, -0.0228,  0.0112,  0.0971, -0.0011,\n",
      "          0.1118, -0.0769,  0.0926,  0.0078,  0.0573,  0.1108, -0.0496,  0.0162,\n",
      "         -0.0495, -0.1094, -0.0988,  0.0329, -0.0454, -0.0822, -0.1109,  0.0005,\n",
      "          0.0921, -0.0656,  0.0429,  0.1134, -0.0367,  0.0947, -0.0165,  0.0481,\n",
      "          0.0992,  0.0598,  0.0503,  0.0702,  0.0065, -0.0975,  0.1244, -0.0998]])), ('output.bias', tensor([-0.0427,  0.0308, -0.0265,  0.0670, -0.0476, -0.0540,  0.0687,  0.1239,\n",
      "        -0.0752, -0.1074]))])}\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for hidden_layers.3.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([50, 100]).\n\tsize mismatch for hidden_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([50]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 50]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-88ebbb046902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 845\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for hidden_layers.3.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([50, 100]).\n\tsize mismatch for hidden_layers.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([50]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 50])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100, 50])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "OrderedDict([('hidden_layers.0.weight', tensor([[ 0.0140,  0.0712,  0.0192,  ...,  0.0321,  0.0505,  0.0593],\n",
      "        [-0.0066, -0.0025,  0.0239,  ...,  0.0068, -0.0220, -0.0112],\n",
      "        [ 0.0025, -0.0248, -0.0232,  ...,  0.0034, -0.0096, -0.0304],\n",
      "        ...,\n",
      "        [ 0.0481, -0.0079,  0.0182,  ...,  0.0329,  0.0253,  0.0467],\n",
      "        [-0.0165,  0.0061,  0.0047,  ...,  0.0083,  0.0291,  0.0063],\n",
      "        [ 0.0051,  0.0386,  0.0439,  ...,  0.0006,  0.0416,  0.0198]])), ('hidden_layers.0.bias', tensor([-7.0398e-02, -3.7007e-02,  1.2524e-02, -6.5606e-02,  1.3192e-02,\n",
      "         1.6500e-03,  5.5836e-03, -4.4033e-02, -9.3538e-03, -1.6790e-02,\n",
      "        -5.8813e-02, -4.0694e-03,  2.0432e-02, -2.0293e-03, -4.1803e-03,\n",
      "        -3.8425e-02,  2.3813e-02, -4.3487e-02, -4.8460e-02,  2.9080e-02,\n",
      "         7.2511e-02, -6.2507e-02, -3.8895e-02, -1.0025e-02,  3.1214e-02,\n",
      "        -1.8273e-02,  9.9938e-03, -2.0657e-02,  4.1378e-02, -3.5472e-02,\n",
      "        -1.3907e-02, -7.3411e-02, -3.3181e-02, -5.7351e-03, -3.9991e-04,\n",
      "        -4.0741e-02, -2.5674e-03, -4.2852e-02, -3.2943e-03, -2.7082e-02,\n",
      "         3.1657e-03, -6.8810e-02, -3.1240e-02,  8.0982e-03, -6.2477e-02,\n",
      "         5.3569e-03,  1.7779e-02, -3.1387e-03, -2.6840e-02, -1.2861e-02,\n",
      "         2.4567e-03, -1.7632e-02, -1.8589e-02, -2.9089e-02, -1.1407e-02,\n",
      "         4.5942e-02, -1.5254e-02, -5.4423e-02, -3.5865e-02, -4.6493e-02,\n",
      "        -5.2120e-02,  5.1643e-03,  1.1339e-02, -4.4891e-02, -1.3205e-02,\n",
      "        -9.9201e-03, -3.8600e-02, -5.5654e-02, -2.5370e-02, -5.9346e-02,\n",
      "        -5.6820e-02, -1.4964e-02,  1.6107e-02,  8.1102e-03, -4.1568e-03,\n",
      "        -2.4049e-02, -2.8763e-02,  2.4835e-05, -3.2038e-02, -3.3593e-02,\n",
      "        -2.5976e-02, -1.5227e-02, -6.2982e-02, -4.5490e-02, -7.2086e-02,\n",
      "        -5.9739e-04, -1.4496e-02, -5.1678e-02, -2.1535e-02, -1.5052e-02,\n",
      "        -1.0294e-02, -3.9395e-02, -2.5660e-03,  8.6135e-03, -3.2931e-02,\n",
      "         3.3580e-03,  1.2041e-02, -3.8604e-02, -4.1715e-02,  2.6900e-02,\n",
      "        -5.7060e-02, -4.7132e-02, -4.0386e-02, -4.3952e-02,  1.1154e-03,\n",
      "        -3.0220e-02,  1.9166e-02, -3.3165e-03,  2.6754e-03, -2.3566e-02,\n",
      "        -3.1009e-02,  2.5510e-02, -4.4805e-02,  6.9493e-03, -4.3126e-02,\n",
      "        -3.1951e-02, -3.7226e-03, -3.8513e-02, -1.0276e-02, -4.2942e-02,\n",
      "        -9.3535e-03, -1.9455e-02, -3.5848e-02, -3.2543e-02, -2.0122e-02,\n",
      "        -4.0692e-03, -4.3194e-02, -5.3956e-03, -3.9234e-02, -3.9120e-03,\n",
      "        -3.5127e-02, -6.0779e-03, -2.4726e-03, -3.8460e-03, -4.2056e-02,\n",
      "        -4.7657e-03, -5.7626e-02, -2.9903e-02, -4.5861e-02, -6.9394e-02,\n",
      "        -6.4065e-02, -4.4575e-02, -7.2730e-02,  1.8794e-02,  2.6479e-02,\n",
      "        -3.4240e-02, -5.5283e-02,  8.3660e-03,  4.9299e-03, -3.7783e-02,\n",
      "        -2.8655e-02,  9.4737e-03,  7.0674e-03, -1.2949e-02,  1.0875e-02,\n",
      "        -3.5796e-02, -9.7538e-03, -3.1609e-02,  5.5998e-03, -7.1469e-03,\n",
      "        -8.8294e-03,  1.4690e-02, -1.1266e-03, -5.7027e-02, -1.3498e-02,\n",
      "         8.4301e-03, -3.2887e-02,  2.8614e-02, -3.3382e-02,  7.5324e-03,\n",
      "        -5.3852e-02, -6.2679e-03, -1.4307e-03, -4.5384e-02, -7.6582e-03,\n",
      "         8.6842e-03, -2.4083e-03,  1.3692e-03, -1.2954e-02,  3.0999e-02,\n",
      "        -1.5976e-02,  2.1904e-03,  2.0007e-03, -2.4804e-02, -1.5303e-02,\n",
      "        -3.9106e-02, -2.9778e-02, -4.3418e-02, -5.7128e-02, -4.4819e-02,\n",
      "        -1.9115e-02, -3.4712e-02, -4.5704e-03,  3.7028e-02,  2.5562e-03,\n",
      "         2.8065e-02, -3.6398e-02, -5.2309e-02, -1.1605e-03,  2.2430e-03,\n",
      "        -4.7098e-02, -2.2623e-02, -1.5973e-02, -2.9957e-02, -6.0307e-02,\n",
      "        -2.7097e-02,  6.1280e-03, -3.7176e-02, -4.0600e-02, -6.4283e-02,\n",
      "        -4.0127e-02,  2.5551e-03,  3.7081e-03, -4.3732e-02, -5.1159e-02,\n",
      "        -4.1297e-03, -3.7845e-02,  1.4332e-02, -5.8370e-03, -7.9470e-02,\n",
      "        -7.1494e-02,  8.4553e-03, -1.2400e-01, -5.3018e-02,  9.7624e-03,\n",
      "        -2.6051e-02, -9.3789e-03,  2.0433e-03,  1.2305e-02, -3.5755e-02,\n",
      "        -4.5452e-02, -4.9867e-02, -5.6410e-02, -2.3008e-02,  2.8597e-02,\n",
      "        -3.2616e-02,  2.6085e-02, -5.1400e-02, -2.9817e-02, -6.6754e-02,\n",
      "        -3.4658e-02, -7.2244e-02, -3.5665e-04, -6.1193e-02, -7.3099e-02,\n",
      "        -2.0620e-02, -3.4517e-02,  4.0742e-02, -2.1137e-02,  6.7513e-04,\n",
      "         2.1632e-02, -1.7851e-02, -4.6988e-03,  2.4545e-02,  2.5748e-02,\n",
      "        -3.3755e-02, -2.4918e-02, -4.8655e-02,  1.5161e-02, -1.3539e-03,\n",
      "         2.4031e-02,  3.4696e-03, -9.3060e-03, -6.1007e-02,  4.8027e-03,\n",
      "         2.0483e-02, -2.4337e-02, -4.9325e-02, -5.6953e-02, -5.8122e-02,\n",
      "         1.4105e-02, -3.2037e-02,  4.0850e-03, -3.8303e-02, -4.3143e-02,\n",
      "        -2.6660e-02, -1.3504e-02, -6.9628e-02, -3.9009e-02, -2.6654e-02,\n",
      "        -1.5698e-02, -2.2331e-02, -2.8119e-02, -5.6064e-02, -3.3787e-02,\n",
      "        -2.7440e-02, -1.9662e-02,  2.7206e-02, -3.8633e-02, -5.2548e-04,\n",
      "        -1.7550e-02,  3.3684e-03, -3.7174e-03,  9.1872e-03, -4.6329e-02,\n",
      "        -4.3696e-02,  1.1217e-02, -4.4531e-02, -8.0482e-03, -2.3266e-02,\n",
      "        -6.1808e-02,  1.5805e-02,  2.7698e-03,  1.0291e-02, -5.4239e-02,\n",
      "        -3.8817e-02, -3.9106e-02,  1.8047e-02, -3.2523e-02, -2.4957e-02,\n",
      "        -3.2723e-02, -6.3950e-02,  5.6876e-03, -2.2251e-02, -2.3065e-02,\n",
      "        -2.3040e-02,  1.2880e-03, -1.3780e-02, -6.3327e-04, -2.3461e-03,\n",
      "         7.4661e-03, -1.9786e-02,  3.4612e-02,  7.4583e-03, -4.5699e-03,\n",
      "        -2.4414e-02, -6.5138e-02, -3.0405e-02, -6.0170e-03, -1.4612e-02,\n",
      "        -1.5091e-02, -3.5882e-02,  8.7744e-03,  3.0865e-02,  6.7464e-03,\n",
      "        -6.2189e-02, -1.0466e-02, -4.5269e-02, -6.7628e-03,  5.5660e-03,\n",
      "        -3.9804e-02, -4.4811e-02,  1.3611e-03, -4.4421e-02, -2.2831e-02,\n",
      "        -2.7116e-02, -2.6003e-02, -2.8152e-02, -5.6041e-02, -3.5614e-02,\n",
      "        -2.6716e-02, -2.5437e-02,  1.0655e-02, -6.4742e-02, -1.8310e-02,\n",
      "        -2.6129e-02, -2.5089e-02, -1.1115e-02,  3.1249e-02, -1.8081e-02,\n",
      "        -6.7440e-03,  7.7622e-03, -1.5827e-02,  2.1500e-02, -7.9966e-02,\n",
      "        -2.3576e-02, -1.2269e-02, -7.3567e-02, -3.8558e-02, -2.6906e-02,\n",
      "         4.3747e-03, -2.5206e-02,  1.5143e-02, -1.3526e-02,  1.7680e-02,\n",
      "        -4.0221e-03, -7.0742e-02, -3.1458e-02, -2.0005e-02, -3.1496e-02,\n",
      "        -5.7860e-02,  1.5707e-02,  8.0985e-04, -9.6096e-03, -1.6084e-02,\n",
      "        -5.2531e-02, -1.5722e-02, -1.1958e-02, -1.9741e-02, -6.1989e-02,\n",
      "        -1.0184e-02,  2.6878e-02, -1.4003e-02, -4.8900e-03, -3.0240e-02,\n",
      "         7.0489e-03, -2.6493e-02,  8.6703e-03, -2.1843e-02, -1.8657e-02,\n",
      "        -1.3395e-02, -5.0677e-02, -3.1372e-02, -7.4380e-02, -3.7472e-02,\n",
      "        -3.8295e-02, -3.4414e-02, -3.7077e-02,  1.0641e-02,  4.9558e-04,\n",
      "        -1.4144e-02, -1.7068e-02, -2.9908e-02, -4.1884e-02,  9.5917e-03,\n",
      "         3.0256e-03, -6.6299e-03,  2.0106e-02, -1.4129e-02, -7.8949e-03,\n",
      "        -7.6558e-02, -1.0821e-02, -1.5514e-02, -2.8520e-02, -4.6536e-02,\n",
      "        -2.5894e-02, -1.7874e-02, -1.1972e-02, -5.7391e-02, -6.6131e-03,\n",
      "        -2.1047e-02, -5.7917e-02, -3.2628e-02, -5.3491e-02, -3.1770e-02,\n",
      "        -1.6728e-02, -2.7435e-02, -6.4594e-02, -4.1502e-02,  4.3861e-02,\n",
      "         4.2792e-03, -2.0120e-02, -4.3333e-02, -1.2362e-02, -1.7195e-02,\n",
      "         9.6671e-03, -2.0451e-02, -4.5997e-03, -1.9058e-02,  1.4459e-02,\n",
      "        -6.1664e-02, -2.7043e-02, -1.3401e-03, -7.1558e-03, -8.1330e-02,\n",
      "        -5.1255e-02, -3.4493e-02, -2.0700e-02, -1.8255e-02, -5.9977e-02,\n",
      "        -3.1028e-02,  2.8493e-03, -2.8428e-02, -1.9183e-02, -3.2224e-02,\n",
      "         3.9491e-02, -1.9916e-02, -3.1075e-03, -8.0135e-03, -2.9202e-02,\n",
      "         1.9059e-02, -3.5178e-02, -2.5151e-02,  1.7952e-02, -3.1526e-02,\n",
      "         5.6150e-03, -4.1643e-02, -2.5070e-02,  1.5757e-02,  5.3863e-03,\n",
      "         7.7461e-03, -3.3184e-02,  8.7958e-03, -8.2052e-03,  2.5228e-02,\n",
      "        -1.1544e-02, -3.5710e-02, -1.1568e-02, -4.3452e-02, -5.9922e-02,\n",
      "         3.8913e-05, -2.0323e-03,  1.3584e-02, -3.0696e-02, -1.0085e-02,\n",
      "         3.7816e-02, -5.7450e-02, -2.6149e-02, -4.2713e-02, -2.1929e-02,\n",
      "        -4.6467e-02, -2.5334e-02, -1.1295e-02, -3.7823e-02, -2.4285e-02,\n",
      "        -3.4311e-03, -1.1889e-03,  2.4299e-03, -2.8641e-02, -4.7740e-02,\n",
      "         1.3509e-02,  5.3546e-04])), ('hidden_layers.1.weight', tensor([[-0.0010,  0.0416, -0.0142,  ..., -0.0518, -0.0773, -0.0204],\n",
      "        [ 0.0042, -0.0063, -0.0110,  ..., -0.0579, -0.0343,  0.0258],\n",
      "        [-0.0641, -0.0481, -0.0604,  ...,  0.0108, -0.0744,  0.0066],\n",
      "        ...,\n",
      "        [-0.0153, -0.0379,  0.0269,  ...,  0.0352, -0.0149,  0.0202],\n",
      "        [-0.0090, -0.0006, -0.0026,  ..., -0.0033,  0.0911,  0.0005],\n",
      "        [ 0.0608, -0.0233,  0.0351,  ...,  0.0190, -0.0036,  0.0133]])), ('hidden_layers.1.bias', tensor([ 0.0203, -0.0025, -0.0085,  0.0051,  0.0456, -0.0238, -0.0080, -0.0329,\n",
      "         0.0065,  0.0570,  0.0045,  0.0516,  0.0007,  0.0214, -0.0380, -0.0079,\n",
      "         0.0556,  0.0637,  0.0660, -0.0372, -0.0156,  0.0696, -0.0359, -0.0506,\n",
      "         0.0559,  0.0010, -0.0964,  0.0159, -0.0176,  0.0385,  0.0372, -0.0510,\n",
      "        -0.0013,  0.0073,  0.0001,  0.0102, -0.0273,  0.0564,  0.0174,  0.0517,\n",
      "        -0.0042,  0.0366,  0.0394, -0.0360,  0.0661, -0.0335,  0.0152,  0.0052,\n",
      "        -0.0162, -0.0303,  0.0434,  0.0639, -0.0220,  0.0295,  0.0176,  0.0234,\n",
      "         0.0030, -0.0141, -0.0562, -0.0294,  0.0358, -0.0938,  0.0373,  0.0173,\n",
      "         0.0006,  0.0333, -0.0434, -0.0035, -0.0949,  0.0025,  0.0408, -0.0539,\n",
      "        -0.0666,  0.0163, -0.0263,  0.0115, -0.0307, -0.0228,  0.0012,  0.0077,\n",
      "         0.0613, -0.0157,  0.0019, -0.0420, -0.0426, -0.0143, -0.0520,  0.0116,\n",
      "        -0.0227, -0.0321, -0.0936, -0.0011,  0.0604, -0.0682, -0.0039,  0.0170,\n",
      "         0.0952, -0.0211, -0.0029, -0.0542,  0.0073,  0.0052,  0.0541,  0.0379,\n",
      "         0.0065,  0.0361,  0.0089, -0.0695,  0.0469,  0.0324,  0.0460,  0.0063,\n",
      "        -0.0066,  0.0371, -0.0352,  0.0757,  0.0340, -0.0598,  0.0480, -0.0233,\n",
      "        -0.0670,  0.0469, -0.0934,  0.0322,  0.0226, -0.0662,  0.0806, -0.0245,\n",
      "         0.0207, -0.0598, -0.0544, -0.0507, -0.0387,  0.0285,  0.0493,  0.0478,\n",
      "         0.0170, -0.0615,  0.0215, -0.0024,  0.0172,  0.0192,  0.0492,  0.0641,\n",
      "        -0.0039,  0.0373, -0.0473, -0.0621,  0.0091,  0.0545,  0.0199,  0.0080,\n",
      "         0.0492,  0.0353,  0.0097,  0.0242,  0.0042,  0.0288, -0.0113,  0.0586,\n",
      "         0.0081, -0.0186,  0.0246, -0.0571,  0.0382, -0.0303,  0.0338, -0.0162,\n",
      "         0.0292,  0.0152,  0.0336,  0.0693,  0.0261,  0.0302, -0.0472,  0.0721,\n",
      "         0.1035, -0.0605, -0.0082, -0.0055, -0.0149, -0.0492,  0.0004,  0.0027,\n",
      "        -0.0097,  0.0288, -0.0350, -0.0158,  0.0679,  0.0178,  0.0427, -0.0405,\n",
      "         0.1190,  0.0182,  0.0742, -0.0203,  0.0172, -0.0183, -0.0178,  0.0497,\n",
      "        -0.0166, -0.0454,  0.0141,  0.0395, -0.0856,  0.0111, -0.0275,  0.0774,\n",
      "        -0.0585,  0.0095,  0.0173, -0.0065, -0.0265,  0.0010,  0.0062,  0.0086,\n",
      "        -0.0052,  0.0735,  0.0227,  0.0297, -0.0069, -0.0179, -0.0052,  0.0608,\n",
      "         0.0081,  0.0210, -0.0768, -0.0714, -0.0048,  0.0214,  0.0868, -0.0148,\n",
      "         0.0405,  0.0102,  0.0785,  0.0335,  0.0118, -0.0325,  0.0208,  0.0632,\n",
      "         0.0091,  0.0553, -0.0023, -0.0463,  0.0017,  0.0336, -0.0068, -0.0087,\n",
      "        -0.0695, -0.0507, -0.0487,  0.0031, -0.0197,  0.0515,  0.0348, -0.0546])), ('hidden_layers.2.weight', tensor([[-0.0876, -0.0612, -0.0334,  ...,  0.0401, -0.1622,  0.0121],\n",
      "        [-0.0605,  0.0276, -0.0295,  ...,  0.0278,  0.0466, -0.0457],\n",
      "        [-0.0221,  0.0171, -0.0256,  ..., -0.0172,  0.0012, -0.0029],\n",
      "        ...,\n",
      "        [ 0.0284, -0.0064,  0.0849,  ...,  0.0112,  0.0522,  0.0353],\n",
      "        [-0.0179, -0.0583, -0.0732,  ...,  0.0239, -0.0737, -0.0007],\n",
      "        [-0.0015,  0.0010,  0.0423,  ...,  0.1021, -0.0256,  0.0389]])), ('hidden_layers.2.bias', tensor([ 0.1982,  0.0415,  0.1191, -0.0853,  0.1391,  0.0487,  0.0141,  0.0778,\n",
      "         0.1240,  0.0736, -0.0199,  0.0573,  0.0352,  0.1138,  0.0622, -0.0843,\n",
      "         0.0227, -0.0089,  0.0543, -0.0447,  0.1067,  0.0413, -0.0140,  0.0411,\n",
      "         0.0279,  0.0048,  0.1548, -0.0177,  0.0029,  0.0029, -0.0842,  0.0235,\n",
      "         0.0721,  0.1220,  0.0939,  0.0410, -0.0470, -0.0050, -0.0128,  0.0460,\n",
      "         0.0543,  0.0933,  0.0138,  0.1601,  0.0347, -0.0349, -0.0084,  0.0210,\n",
      "        -0.0068,  0.0697,  0.0807, -0.0862,  0.0600,  0.0768, -0.0123,  0.0616,\n",
      "         0.0177,  0.0942,  0.0405,  0.0799,  0.0953, -0.0329, -0.0888,  0.0146,\n",
      "         0.0071,  0.0527,  0.0613,  0.0420,  0.0430,  0.0086, -0.0053, -0.0366,\n",
      "         0.1036,  0.0602,  0.0436, -0.0668,  0.0264,  0.0502,  0.0350,  0.1133,\n",
      "         0.0229,  0.0749, -0.0297,  0.0962, -0.1065,  0.0944,  0.0673,  0.1343,\n",
      "         0.0844,  0.0324,  0.0311,  0.1500, -0.0339,  0.0150,  0.1525, -0.0995,\n",
      "         0.0957, -0.0214,  0.0960,  0.0060,  0.0041,  0.0571,  0.0465, -0.0937,\n",
      "         0.0170,  0.0448,  0.1218,  0.0434,  0.0483,  0.0075, -0.0946,  0.0075,\n",
      "         0.0671, -0.0612,  0.0942,  0.0464,  0.0205,  0.0189,  0.0022,  0.0731,\n",
      "         0.0543,  0.0514, -0.0368,  0.0677,  0.0007,  0.1575,  0.0325, -0.0185])), ('hidden_layers.3.weight', tensor([[ 0.0197, -0.0463, -0.0489,  ..., -0.0146, -0.0006, -0.1129],\n",
      "        [ 0.0575,  0.0118, -0.0734,  ...,  0.0475, -0.0013,  0.1130],\n",
      "        [-0.0092, -0.0816,  0.1241,  ..., -0.0524,  0.0603, -0.0786],\n",
      "        ...,\n",
      "        [ 0.1322,  0.0217,  0.0308,  ...,  0.1114,  0.0658, -0.0306],\n",
      "        [-0.0522,  0.0821, -0.0361,  ...,  0.0773,  0.0614,  0.0230],\n",
      "        [-0.0021,  0.0182,  0.0711,  ...,  0.0027,  0.0104, -0.1369]])), ('hidden_layers.3.bias', tensor([ 0.0988,  0.0042,  0.1238,  0.3065,  0.0048,  0.1439, -0.0010,  0.1543,\n",
      "        -0.0087,  0.1446,  0.1300,  0.0736, -0.0080,  0.1359,  0.1424,  0.2344,\n",
      "         0.0805,  0.1196,  0.0444,  0.2507,  0.0008,  0.0110,  0.1828,  0.0408,\n",
      "         0.1107,  0.1370,  0.1001,  0.0649,  0.2836, -0.1203,  0.0164,  0.1577,\n",
      "         0.0603, -0.0069,  0.0485,  0.0554,  0.1350,  0.0646, -0.0629,  0.2321,\n",
      "         0.0225,  0.0167,  0.0015,  0.2406,  0.1096,  0.0960,  0.1920,  0.1861,\n",
      "         0.2267,  0.1379,  0.1161,  0.1153, -0.1418,  0.1295, -0.0507,  0.1648,\n",
      "         0.0460,  0.0268,  0.0551,  0.2273,  0.2585,  0.1470,  0.2234,  0.1433])), ('output.weight', tensor([[-0.0555, -0.0379, -0.0164,  0.0891,  0.0081, -0.1191, -0.0367, -0.1044,\n",
      "         -0.1185,  0.0352, -0.0021, -0.0144,  0.0448, -0.0257, -0.2321,  0.0672,\n",
      "         -0.1760, -0.0184, -0.0721, -0.0117, -0.0477, -0.0130,  0.0391, -0.0772,\n",
      "         -0.0857, -0.1227, -0.1593,  0.0461,  0.0705, -0.0467,  0.0076,  0.0522,\n",
      "          0.0509,  0.0822,  0.0761, -0.1189, -0.0142,  0.0805, -0.0496,  0.0585,\n",
      "          0.0381, -0.1012, -0.0494,  0.1075, -0.0276,  0.0094,  0.1032, -0.1298,\n",
      "          0.0563, -0.0907,  0.0645, -0.0826, -0.1308,  0.0853, -0.0345, -0.1142,\n",
      "          0.0932, -0.0209, -0.1014, -0.0482, -0.1220, -0.0899,  0.0994,  0.0748],\n",
      "        [-0.1349,  0.1032, -0.1788, -0.0425, -0.1969, -0.1412,  0.0872, -0.1246,\n",
      "          0.0850, -0.0023, -0.1541,  0.0600, -0.0370, -0.0762, -0.2157, -0.0882,\n",
      "         -0.0155,  0.0494,  0.0642, -0.2091,  0.0698,  0.1163,  0.0387,  0.0554,\n",
      "         -0.0711,  0.0515, -0.0015, -0.1490, -0.1123,  0.0984, -0.1951,  0.0477,\n",
      "          0.0974,  0.0077,  0.0109, -0.0360,  0.0387,  0.0580,  0.0359,  0.0517,\n",
      "         -0.0362, -0.0625,  0.0703, -0.0505,  0.1065, -0.1322, -0.0555, -0.1761,\n",
      "         -0.1620,  0.0423, -0.0355, -0.1342,  0.1033, -0.0393,  0.1197, -0.1762,\n",
      "          0.0840,  0.0302, -0.2634, -0.1479, -0.0942, -0.1587,  0.0507, -0.1317],\n",
      "        [ 0.0156, -0.0028,  0.0573,  0.0803, -0.1117,  0.0831, -0.0577, -0.0168,\n",
      "         -0.0559, -0.2176, -0.1678, -0.0128, -0.0791, -0.1102, -0.0561,  0.0417,\n",
      "         -0.2365, -0.0181,  0.0847,  0.0453, -0.0659, -0.0107, -0.0497, -0.0212,\n",
      "         -0.1846,  0.0944, -0.1876,  0.0774,  0.0242, -0.0424,  0.0155, -0.0404,\n",
      "         -0.1474,  0.0708, -0.0516,  0.0808, -0.1093,  0.0298, -0.1281,  0.0481,\n",
      "         -0.0054, -0.2746, -0.0206,  0.1287, -0.0288,  0.0353,  0.0033, -0.0069,\n",
      "          0.0375,  0.0721, -0.0931,  0.0518, -0.0609,  0.0494, -0.0810, -0.1226,\n",
      "         -0.0239, -0.1904,  0.0006,  0.0917,  0.0735,  0.1124,  0.0063,  0.1020],\n",
      "        [-0.2296,  0.0814, -0.1017,  0.0686, -0.0936,  0.0278, -0.0410, -0.1747,\n",
      "         -0.0386,  0.0847, -0.0649, -0.1359,  0.0108,  0.0312, -0.2487,  0.0568,\n",
      "         -0.0888,  0.0528,  0.0826, -0.0118,  0.0757, -0.0324,  0.0610,  0.0388,\n",
      "          0.0087, -0.0521,  0.0209, -0.0752,  0.0428, -0.0713, -0.2056,  0.1048,\n",
      "          0.0831, -0.0704, -0.0039, -0.1441,  0.0403, -0.0110, -0.1541,  0.0696,\n",
      "          0.0397, -0.1523,  0.0483,  0.0989,  0.0257, -0.0861,  0.0889, -0.2194,\n",
      "         -0.0703,  0.0457,  0.0965, -0.0883, -0.0847,  0.0624, -0.0600, -0.2136,\n",
      "          0.1018, -0.0862, -0.1975, -0.0404, -0.1039, -0.0073,  0.1014, -0.0891],\n",
      "        [-0.0977,  0.0610,  0.0464,  0.0680, -0.0167,  0.0833, -0.0260,  0.0053,\n",
      "         -0.0539, -0.1329, -0.3326,  0.0110, -0.1908, -0.1190, -0.0727, -0.0844,\n",
      "         -0.2601,  0.0227,  0.1027,  0.0601,  0.0369, -0.0454,  0.0334,  0.0406,\n",
      "         -0.1642,  0.0935, -0.1101,  0.0740, -0.0473,  0.0145,  0.0167,  0.0617,\n",
      "         -0.0436,  0.0864, -0.0038,  0.0711,  0.0173, -0.1325, -0.2355,  0.0613,\n",
      "         -0.1176, -0.1786,  0.0426,  0.0036, -0.0267, -0.1552, -0.0009, -0.1394,\n",
      "         -0.0557,  0.0784, -0.0275,  0.0589, -0.0797, -0.0547, -0.0830, -0.1471,\n",
      "         -0.0189, -0.1661, -0.1164,  0.0960,  0.0471,  0.1044,  0.0695, -0.0257],\n",
      "        [ 0.0715, -0.1633, -0.1310, -0.3727,  0.0598,  0.0968,  0.0084,  0.1142,\n",
      "         -0.0872,  0.0964, -0.0157, -0.0599,  0.0831,  0.0939,  0.0496, -0.0094,\n",
      "          0.0545, -0.1242, -0.1041, -0.4196,  0.0229,  0.0702, -0.2213, -0.2124,\n",
      "          0.1085, -0.1733,  0.0862, -0.0921, -0.2275, -0.0121, -0.1266,  0.0147,\n",
      "         -0.0940, -0.1731, -0.1656, -0.1014,  0.0281,  0.0880,  0.0884, -0.3064,\n",
      "          0.0469, -0.0067, -0.0714,  0.0151,  0.0620,  0.0865, -0.2551,  0.0866,\n",
      "         -0.0042, -0.3567,  0.1300, -0.0205, -0.1144,  0.0301, -0.0799,  0.0047,\n",
      "         -0.1668,  0.0673,  0.0806, -0.0011,  0.0424,  0.0782, -0.2255, -0.0801],\n",
      "        [-0.0458,  0.0114,  0.0520,  0.0643,  0.0230,  0.0739, -0.2424,  0.0005,\n",
      "         -0.0547, -0.0839, -0.0355,  0.0217,  0.0082, -0.1159, -0.1454,  0.0561,\n",
      "         -0.2533, -0.0724,  0.0854,  0.0544, -0.0923, -0.0590,  0.0233, -0.0468,\n",
      "         -0.1687,  0.0619, -0.2261,  0.0671,  0.0663, -0.1226, -0.0114,  0.0095,\n",
      "         -0.0109,  0.0478,  0.0684,  0.0121, -0.0915,  0.0558, -0.1028,  0.0494,\n",
      "          0.0167, -0.1497, -0.0613,  0.1107, -0.0671, -0.0328,  0.0984, -0.0450,\n",
      "          0.0424,  0.0688,  0.0333,  0.0514, -0.1742,  0.0672, -0.1162, -0.0632,\n",
      "          0.0868, -0.2272, -0.0786,  0.0972,  0.0579,  0.0887,  0.0900,  0.0661],\n",
      "        [ 0.0608, -0.1213, -0.0683, -0.3306,  0.0783,  0.0694,  0.0903,  0.1085,\n",
      "         -0.1352,  0.0906,  0.0369,  0.0365,  0.0735,  0.0047,  0.0439, -0.1391,\n",
      "          0.0359, -0.1200, -0.2530, -0.4234, -0.1057,  0.0084, -0.2804, -0.0824,\n",
      "          0.1182, -0.1896,  0.1251, -0.1091, -0.3300,  0.0249, -0.1637, -0.1387,\n",
      "          0.0451, -0.0334, -0.0715, -0.0065, -0.0807, -0.0033,  0.0950, -0.2733,\n",
      "         -0.0402,  0.0575,  0.0340, -0.1771, -0.1225, -0.0483, -0.3016,  0.1069,\n",
      "         -0.1575, -0.4733,  0.1213, -0.0633, -0.1262, -0.0511,  0.0115,  0.0643,\n",
      "         -0.0553,  0.0762,  0.0944,  0.0531,  0.0613, -0.0633, -0.1602, -0.2021],\n",
      "        [ 0.0324, -0.0942, -0.0675, -0.0833,  0.0868, -0.1710, -0.0918,  0.0910,\n",
      "         -0.0483,  0.0572, -0.0695,  0.1054,  0.0934, -0.0742, -0.0909,  0.0736,\n",
      "         -0.0255,  0.1040, -0.1240, -0.0575, -0.0339, -0.2007,  0.0444, -0.1469,\n",
      "         -0.0589,  0.0471, -0.0617,  0.0764,  0.0396, -0.0858,  0.0231, -0.0444,\n",
      "         -0.0134, -0.1994,  0.0592,  0.0378, -0.0457, -0.1601, -0.0577, -0.0827,\n",
      "         -0.1359,  0.0406, -0.0060,  0.0121,  0.0814,  0.0708, -0.0304,  0.0675,\n",
      "         -0.1465, -0.0565, -0.0725, -0.0635, -0.0146,  0.0962, -0.1065,  0.0536,\n",
      "         -0.1234,  0.0611, -0.1172, -0.1188, -0.0829, -0.0492, -0.0906,  0.0576],\n",
      "        [ 0.0580, -0.0444, -0.0072, -0.3404, -0.0914,  0.1033,  0.0989,  0.0502,\n",
      "         -0.0398, -0.0201,  0.0414,  0.0501, -0.0245,  0.0022,  0.0582, -0.2316,\n",
      "          0.0553, -0.2003, -0.1538, -0.3997,  0.0180,  0.0769, -0.2745, -0.1802,\n",
      "         -0.0554, -0.0642,  0.1232, -0.0257, -0.2179,  0.0294, -0.0610, -0.0975,\n",
      "          0.0456,  0.0124, -0.0674,  0.0158, -0.0850,  0.1216,  0.1118, -0.3683,\n",
      "          0.0586,  0.0621, -0.0956, -0.1521, -0.1011,  0.0565, -0.2829, -0.0049,\n",
      "         -0.0585, -0.3007,  0.1148, -0.2140, -0.0803, -0.1812, -0.0374,  0.0495,\n",
      "         -0.1611,  0.0594,  0.0895, -0.0513, -0.1422,  0.0424, -0.1910, -0.0995]])), ('output.bias', tensor([-0.1634, -0.4143,  0.0845,  0.0855,  0.0573,  0.0718,  0.0519,  0.0026,\n",
      "         0.0056, -0.1219]))])\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
